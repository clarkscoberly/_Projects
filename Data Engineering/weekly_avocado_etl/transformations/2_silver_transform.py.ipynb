{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ef1cf8-edf9-4da0-9ace-f0a5c1c6e280",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------\n",
    "# Config\n",
    "# -----------------------\n",
    "SILVER_DB = \"silver\"\n",
    "QUARANTINE_TABLE = f\"{SILVER_DB}.quarantine\"\n",
    "VALID_FERT_TYPES = [\"Heavy Metal\", \"Organic\", \"Inorganic\", \"Dry\", \"Toxic\"]\n",
    "JOB_RUN_ID = str(uuid.uuid4())\n",
    "INGEST_TS = F.current_timestamp()\n",
    "\n",
    "# Columns to drop after renaming\n",
    "COLUMN_RENAMES = {\n",
    "    \"consumerid\": \"consumer_id\",\n",
    "    \"purchaseid\": \"purchase_id\",\n",
    "    \"fertilizerid\": \"fertilizer_id\"\n",
    "}\n",
    "\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {SILVER_DB}\")\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {QUARANTINE_TABLE} (\n",
    "    source_table STRING,\n",
    "    pk_value STRING,\n",
    "    fk_values STRING,\n",
    "    full_row STRING,\n",
    "    failure_reason STRING,\n",
    "    ingest_timestamp TIMESTAMP,\n",
    "    job_run_id STRING,\n",
    "    occurrence_count INT\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# -----------------------\n",
    "# Helpers\n",
    "# -----------------------\n",
    "def df_has_rows(df):\n",
    "    return bool(df.take(1))\n",
    "\n",
    "def write_to_quarantine(df, source_table, pk_col, fk_expr=None, failure_reason=\"invalid\"):\n",
    "    if not df_has_rows(df):\n",
    "        return\n",
    "    if fk_expr is None:\n",
    "        df = df.withColumn(\"fk_values\", F.lit(None).cast(\"string\"))\n",
    "    else:\n",
    "        df = df.withColumn(\"fk_values\", fk_expr.cast(\"string\"))\n",
    "\n",
    "    full_row_col = F.to_json(F.struct([F.col(c) for c in df.columns])).alias(\"full_row\")\n",
    "    df_to_write = df.select(\n",
    "        F.lit(source_table).alias(\"source_table\"),\n",
    "        F.col(pk_col).cast(\"string\").alias(\"pk_value\"),\n",
    "        F.col(\"fk_values\"),\n",
    "        full_row_col,\n",
    "        F.lit(failure_reason).alias(\"failure_reason\")\n",
    "    ).withColumn(\"ingest_timestamp\", INGEST_TS).withColumn(\"job_run_id\", F.lit(JOB_RUN_ID))\n",
    "\n",
    "    df_to_write.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(QUARANTINE_TABLE)\n",
    "\n",
    "def dedupe(df, pk_cols, orderby=None):\n",
    "    if orderby:\n",
    "        window_spec = Window.partitionBy(pk_cols).orderBy(orderby)\n",
    "        return df.withColumn(\"_rn\", F.row_number().over(window_spec)).filter(F.col(\"_rn\") == 1).drop(\"_rn\")\n",
    "    else:\n",
    "        return df.dropDuplicates(pk_cols)\n",
    "\n",
    "def upsert_delta(target_table: str, df, key_cols):\n",
    "    if not df_has_rows(df):\n",
    "        return\n",
    "\n",
    "    if not spark.catalog.tableExists(target_table):\n",
    "        df.write.format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(target_table)\n",
    "        return\n",
    "\n",
    "    # Align source DF to target schema\n",
    "    target_schema = spark.table(target_table).schema\n",
    "    target_cols = [f.name for f in target_schema.fields]\n",
    "    df_aligned = df.select(\n",
    "        [F.col(c).alias(c) if c in df.columns else F.lit(None).alias(c) for c in target_cols]\n",
    "    )\n",
    "\n",
    "    merge_cond = \" AND \".join([f\"t.{k} = s.{k}\" for k in key_cols if k in df_aligned.columns])\n",
    "\n",
    "    DeltaTable.forName(spark, target_table).alias(\"t\")\\\n",
    "        .merge(df_aligned.alias(\"s\"), merge_cond)\\\n",
    "        .whenMatchedUpdateAll()\\\n",
    "        .whenNotMatchedInsertAll()\\\n",
    "        .execute()\n",
    "\n",
    "def drop_replaced_columns(df, rename_map):\n",
    "    old_cols = [old for old,new in rename_map.items() if old in df.columns and old != new]\n",
    "    if old_cols:\n",
    "        df = df.drop(*old_cols)\n",
    "    return df\n",
    "\n",
    "# -----------------------\n",
    "# Load bronze tables\n",
    "# -----------------------\n",
    "bronze_tables = {\n",
    "    \"consumer\": \"bronze.consumer\",\n",
    "    \"purchase\": \"bronze.purchase\",\n",
    "    \"avocado\": \"bronze.avocado\",\n",
    "    \"fertilizer\": \"bronze.fertilizer\"\n",
    "}\n",
    "bronze_df = {k: spark.table(v) for k,v in bronze_tables.items()}\n",
    "\n",
    "# -----------------------\n",
    "# Transformations\n",
    "# -----------------------\n",
    "\n",
    "## Consumer\n",
    "consumer = bronze_df[\"consumer\"].withColumn(\"consumer_id\", F.col(\"consumerid\").cast(\"long\"))\\\n",
    "                                 .withColumn(\"sex\", F.initcap(F.trim(F.col(\"Sex\"))))\\\n",
    "                                 .withColumn(\"age\", F.col(\"age\").cast(\"int\"))\n",
    "\n",
    "consumer_valid = consumer.filter(F.col(\"consumer_id\").isNotNull() & F.col(\"age\").between(0,130))\n",
    "consumer_valid = drop_replaced_columns(consumer_valid, COLUMN_RENAMES)\n",
    "consumer_malformed = consumer.filter(F.col(\"consumer_id\").isNull() | ~F.col(\"age\").between(0,130))\n",
    "\n",
    "upsert_delta(f\"{SILVER_DB}.validated_consumer\", consumer_valid, [\"consumer_id\"])\n",
    "write_to_quarantine(consumer_malformed, \"consumer\", \"consumer_id\", failure_reason=\"invalid consumer\")\n",
    "\n",
    "## Purchase\n",
    "purchase = bronze_df[\"purchase\"].withColumn(\"purchase_id\", F.col(\"purchaseid\").cast(\"long\"))\\\n",
    "                                 .withColumn(\"consumer_id\", F.col(\"consumerid\").cast(\"long\"))\\\n",
    "                                 .withColumn(\"graphed_date\", F.try_to_date(F.substring(\"graphed_date\",1,10), \"yyyy-MM-dd\"))\\\n",
    "                                 .withColumn(\"avocado_bunch_id\", F.col(\"avocado_bunch_id\").cast(\"int\"))\\\n",
    "                                 .withColumn(\"price_index\", F.col(\"price_index\").cast(\"int\"))\\\n",
    "                                 .withColumn(\"reporting_year\", F.col(\"reporting_year\").cast(\"int\"))\\\n",
    "                                 .withColumn(\"grocery_store_id\", F.col(\"grocery_store_id\").cast(\"int\"))\n",
    "\n",
    "purchase_valid = purchase.filter(F.col(\"purchase_id\").isNotNull() & F.col(\"consumer_id\").isNotNull())\n",
    "window_spec = Window.partitionBy(\"purchase_id\").orderBy(F.col(\"graphed_date\").desc_nulls_last())\n",
    "purchase_valid_deduped = purchase_valid.withColumn(\"_rn\", F.row_number().over(window_spec))\\\n",
    "                                      .filter(F.col(\"_rn\") == 1).drop(\"_rn\")\n",
    "purchase_valid_deduped = drop_replaced_columns(purchase_valid_deduped, COLUMN_RENAMES)\n",
    "purchase_malformed = purchase.filter(F.col(\"purchase_id\").isNull() | F.col(\"consumer_id\").isNull())\n",
    "\n",
    "upsert_delta(f\"{SILVER_DB}.validated_purchase\", purchase_valid_deduped, [\"purchase_id\"])\n",
    "write_to_quarantine(purchase_malformed, \"purchase\", \"purchase_id\",\n",
    "                    F.concat(F.lit(\"consumer_id=\"), F.col(\"consumerid\").cast(\"string\")),\n",
    "                    \"invalid purchase\")\n",
    "\n",
    "## Avocado\n",
    "avocado = bronze_df[\"avocado\"].withColumn(\"purchase_id\", F.col(\"purchaseid\").cast(\"long\"))\\\n",
    "                               .withColumn(\"consumer_id\", F.col(\"consumerid\").cast(\"long\"))\\\n",
    "                               .withColumn(\"born_date\", F.try_to_date(F.substring(\"born_date\",1,10), \"yyyy-MM-dd\"))\\\n",
    "                               .withColumn(\"picked_date\", F.try_to_date(F.substring(\"picked_date\",1,10), \"yyyy-MM-dd\"))\\\n",
    "                               .withColumn(\"sold_date\", F.try_to_date(F.substring(\"sold_date\",1,10), \"yyyy-MM-dd\"))\\\n",
    "                               .withColumn(\"avocado_ripe_index\", F.col(\"ripe_index_when_picked\").cast(\"int\"))\\\n",
    "                               .withColumn(\"avocado_bunch_id\", F.col(\"avocado_bunch_id\").cast(\"int\"))\n",
    "\n",
    "avocado_valid = avocado.filter(\n",
    "    F.col(\"purchase_id\").isNotNull() & F.col(\"consumer_id\").isNotNull() &\n",
    "    (F.col(\"picked_date\") >= F.col(\"born_date\")) &\n",
    "    (F.col(\"sold_date\") >= F.col(\"picked_date\"))\n",
    ")\n",
    "avocado_valid = drop_replaced_columns(avocado_valid, COLUMN_RENAMES)\n",
    "\n",
    "avocado_malformed = avocado.filter(\n",
    "    F.col(\"purchase_id\").isNull() | F.col(\"consumer_id\").isNull() |\n",
    "    (F.col(\"picked_date\") < F.col(\"born_date\")) |\n",
    "    (F.col(\"sold_date\") < F.col(\"picked_date\"))\n",
    ")\n",
    "\n",
    "upsert_delta(f\"{SILVER_DB}.validated_avocado\", avocado_valid, [\"purchase_id\"])\n",
    "write_to_quarantine(avocado_malformed, \"avocado\", \"purchase_id\",\n",
    "                    F.concat(F.lit(\"consumer_id=\"), F.col(\"consumerid\").cast(\"string\")),\n",
    "                    \"invalid avocado dates or missing identifiers\")\n",
    "\n",
    "## Fertilizer\n",
    "fert = bronze_df[\"fertilizer\"].withColumn(\"purchase_id\", F.col(\"purchaseid\").cast(\"long\"))\\\n",
    "                               .withColumn(\"consumer_id\", F.col(\"consumerid\").cast(\"long\"))\\\n",
    "                               .withColumn(\"fertilizer_id\", F.col(\"fertilizerid\").cast(\"long\"))\\\n",
    "                               .withColumn(\"fertilizer_type\", F.initcap(F.trim(F.col(\"type\"))))\\\n",
    "                               .withColumn(\"mg\", F.col(\"mg\"))\n",
    "\n",
    "fert_valid = fert.filter(F.col(\"fertilizer_id\").isNotNull() &\n",
    "                         F.col(\"purchase_id\").isNotNull() &\n",
    "                         F.col(\"fertilizer_type\").isin(*VALID_FERT_TYPES))\\\n",
    "                 .dropDuplicates([\"fertilizer_id\"])\n",
    "fert_valid = drop_replaced_columns(fert_valid, COLUMN_RENAMES)\n",
    "\n",
    "fert_malformed = fert.filter(F.col(\"fertilizer_id\").isNull() |\n",
    "                             F.col(\"purchase_id\").isNull() |\n",
    "                             ~F.col(\"fertilizer_type\").isin(*VALID_FERT_TYPES))\n",
    "\n",
    "upsert_delta(f\"{SILVER_DB}.validated_fertilizer\", fert_valid, [\"fertilizer_id\"])\n",
    "write_to_quarantine(fert_malformed, \"fertilizer\", \"fertilizer_id\",\n",
    "                    F.concat(F.lit(\"purchase_id=\"), F.col(\"purchaseid\").cast(\"string\"),\n",
    "                             F.lit(\",consumer_id=\"), F.col(\"consumerid\").cast(\"string\")),\n",
    "                    \"invalid fertilizer type or missing ids\")\n",
    "\n",
    "# -----------------------\n",
    "# Referential Integrity\n",
    "# -----------------------\n",
    "if spark.catalog.tableExists(f\"{SILVER_DB}.validated_purchase\") and spark.catalog.tableExists(f\"{SILVER_DB}.validated_consumer\"):\n",
    "    val_purchase = spark.table(f\"{SILVER_DB}.validated_purchase\")\n",
    "    val_consumer = spark.table(f\"{SILVER_DB}.validated_consumer\").select(\"consumer_id\").distinct()\n",
    "    missing_consumers = val_purchase.join(val_consumer, on=\"consumer_id\", how=\"left_anti\")\n",
    "    write_to_quarantine(missing_consumers, \"purchase\", \"purchase_id\",\n",
    "                        F.concat(F.lit(\"consumer_id=\"), F.col(\"consumer_id\").cast(\"string\")),\n",
    "                        \"referential integrity: missing consumer\")\n",
    "\n",
    "# -----------------------\n",
    "# Aggregate Quarantine\n",
    "# -----------------------\n",
    "quarantine_agg_df = (spark.table(QUARANTINE_TABLE)\n",
    "                     .groupBy(\"full_row\",\"failure_reason\")\n",
    "                     .agg(F.count(\"*\").alias(\"occurrence_count\")))\n",
    "\n",
    "quarantine_agg_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(QUARANTINE_TABLE)\n",
    "\n",
    "print(\"Silver transformation complete.\")\n",
    "print(f\"job_run_id = {JOB_RUN_ID}; time = {datetime.utcnow().isoformat()}Z\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26167fb6-d49d-4d3a-a310-e492889a4541",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5095379211328508,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "2_silver_transform.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
