{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00db5aeb-dcef-4f48-9dfa-bd0ba0a6d466",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# generate_gold_output.py\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "config_path = \"/Workspace/Users/clarkscoberly@gmail.com/config.yaml\"\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "S3_BUCKET = config[\"s3_bucket\"].rstrip(\"/\")         \n",
    "OUTPUT_PREFIX = config.get(\"output_prefix\", \"outputs\")  \n",
    "ITERATION = config.get(\"iteration\", 1)             \n",
    "\n",
    "# -------------------------\n",
    "# Load Silver Tables\n",
    "# -------------------------\n",
    "pur = spark.table(\"silver.validated_purchase\").alias(\"p\") \n",
    "con = spark.table(\"silver.validated_consumer\").alias(\"c\")\n",
    "avo = spark.table(\"silver.validated_avocado\").alias(\"a\")\n",
    "fer = spark.table(\"silver.validated_fertilizer\").alias(\"f\")\n",
    "\n",
    "# -------------------------\n",
    "# Joins (explicit, unambiguous)\n",
    "# -------------------------\n",
    "# Use purchase.consumer_id as the canonical consumer_id to avoid ambiguity.\n",
    "joined = (\n",
    "    pur.join(con, F.col(\"p.consumer_id\") == F.col(\"c.consumer_id\"), how=\"inner\")\n",
    "       .select(\n",
    "           F.col(\"p.purchase_id\").alias(\"purchase_id\"),\n",
    "           F.col(\"c.consumer_id\").alias(\"consumer_id\"),   \n",
    "           F.col(\"c.sex\").alias(\"Sex\"),                   \n",
    "           F.col(\"c.age\").alias(\"age\"),\n",
    "           F.col(\"p.graphed_date\").alias(\"graphed_date\"),\n",
    "           F.col(\"p.avocado_bunch_id\").alias(\"avocado_bunch_id\")\n",
    "       )\n",
    ")\n",
    "\n",
    "joined_avo = joined.join(avo, on=F.col(\"purchase_id\") == F.col(\"a.purchase_id\"), how=\"left\") \\\n",
    "                   .select(\n",
    "                       \"purchase_id\",\n",
    "                       \"consumer_id\",\n",
    "                       \"Sex\",\n",
    "                       \"age\",\n",
    "                       \"graphed_date\",\n",
    "                       \"avocado_bunch_id\",\n",
    "                       F.col(\"a.born_date\").alias(\"born_date\"),\n",
    "                       F.col(\"a.picked_date\").alias(\"picked_date\"),\n",
    "                       F.col(\"a.sold_date\").alias(\"sold_date\"),\n",
    "                       F.col(\"a.avocado_ripe_index\").alias(\"avocado_ripe_index\")\n",
    "                   )\n",
    "\n",
    "final_df = joined_avo.join(fer, on=F.col(\"purchase_id\") == F.col(\"f.purchase_id\"), how=\"left\") \\\n",
    "    .select(\n",
    "        F.col(\"purchase_id\"),\n",
    "        F.col(\"consumer_id\"),\n",
    "        F.col(\"Sex\"),\n",
    "        F.col(\"age\"),\n",
    "        # Derived metrics (use datediff; keep null if dates missing)\n",
    "        F.datediff(F.col(\"sold_date\"), F.col(\"born_date\")).alias(\"avocado_days_sold\"),\n",
    "        F.col(\"avocado_ripe_index\"),\n",
    "        F.datediff(F.col(\"picked_date\"), F.col(\"born_date\")).alias(\"avocado_days_picked\"),\n",
    "        F.col(\"fertilizer_type\")\n",
    "    )\n",
    "\n",
    "# -------------------------\n",
    "# Persist Gold Delta table (authoritative)\n",
    "# -------------------------\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "(\n",
    "    final_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"merge\")         \n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(\"gold.gold_output\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# CSV Output: file spec\n",
    "# -------------------------\n",
    "\n",
    "date_str = datetime.now().strftime(\"%Y%m%d\")   # e.g. 20251121\n",
    "filename = f\"target_{ITERATION}_{date_str}.csv\"\n",
    "\n",
    "# Output path staging (will write into this folder, then rename the part file)\n",
    "output_staging = f\"{S3_BUCKET}/{OUTPUT_PREFIX}/{filename}.tmp/\"\n",
    "final_output_path = f\"{S3_BUCKET}/{OUTPUT_PREFIX}/{filename}\"\n",
    "\n",
    "# Ensure header columns and spellings match spec exactly:\n",
    "# Header row will be: consumer_id|Sex|age|avocado_days_sold|avocado_ripe_index|avocado_days_picked|fertilizer_type\n",
    "ordered_df = final_df.select(\n",
    "    \"consumer_id\",\n",
    "    \"Sex\",\n",
    "    \"age\",\n",
    "    \"avocado_days_sold\",\n",
    "    \"avocado_ripe_index\",\n",
    "    \"avocado_days_picked\",\n",
    "    \"fertilizer_type\"\n",
    ")\n",
    "\n",
    "# Coalesce to single file (if dataset is small enough; for large data this is NOT recommended).\n",
    "single_csv_df = ordered_df.coalesce(1)\n",
    "\n",
    "single_csv_df.write \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .option(\"quote\", '\"') \\\n",
    "    .option(\"escape\", '\"') \\\n",
    "    .option(\"encoding\", \"ASCII\") \\\n",
    "    .option(\"lineSep\", \"\\n\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .csv(output_staging)\n",
    "\n",
    "# Move / rename the produced part-*.csv to the target filename (Databricks helper).\n",
    "# WARNING: dbutils is Databricks-specific.\n",
    "try:\n",
    "    files = dbutils.fs.ls(output_staging)\n",
    "    part_file = None\n",
    "    for f in files:\n",
    "        # find the first CSV part file (part-*.csv)\n",
    "        if re.search(r\"part-.*\\.csv$\", f.name):\n",
    "            part_file = f.path\n",
    "            break\n",
    "\n",
    "    if part_file is None:\n",
    "        raise Exception(f\"No part csv found in {output_staging}; list: {files}\")\n",
    "\n",
    "    # If an output file already exists, remove it first (overwrite behavior)\n",
    "    if len(dbutils.fs.ls(f\"{S3_BUCKET}/{OUTPUT_PREFIX}\")) > 0:\n",
    "        # Remove existing file with same name (if present)\n",
    "        try:\n",
    "            dbutils.fs.rm(final_output_path)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    dbutils.fs.mv(part_file, final_output_path)\n",
    "    # cleanup temp folder\n",
    "    dbutils.fs.rm(output_staging, recurse=True)\n",
    "    print(f\"Wrote CSV to: {final_output_path}\")\n",
    "\n",
    "# TODO Add in email chains to signify ETL was successful to determined owners\n",
    "email()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6331fa8-80be-42ca-bba5-c90b8c46ab1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "3_gold_generate.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
