{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3dba279-d7ba-47b3-a451-f6d08d4b95ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# generate_gold_output.py\n",
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime\n",
    "from pyspark.sql.window import Window\n",
    "import yaml\n",
    "import re\n",
    "\n",
    "gold_table = \"gold.gold_output\"\n",
    "\n",
    "if spark.catalog.tableExists(gold_table):\n",
    "    # Option 1: Truncate existing table\n",
    "    spark.sql(f\"TRUNCATE TABLE {gold_table}\")\n",
    "\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "config_path = \"/Workspace/Users/clarkscoberly@gmail.com/_Projects/Data Engineering/weekly_avocado_etl/0_config.yaml\"\n",
    "with open(config_path) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "S3_BUCKET = config[\"s3_bucket\"].rstrip(\"/\")\n",
    "OUTPUT_PREFIX = config.get(\"output_prefix\", \"outputs\")\n",
    "ITERATION = config.get(\"iteration\", 1)\n",
    "\n",
    "# -------------------------\n",
    "# Load Silver Tables\n",
    "# -------------------------\n",
    "pur = spark.table(\"silver.validated_purchase\")\n",
    "con = spark.table(\"silver.validated_consumer\")\n",
    "avo = spark.table(\"silver.validated_avocado\")\n",
    "fer = spark.table(\"silver.validated_fertilizer\")\n",
    "\n",
    "# -------------------------\n",
    "# Join chain (left joins)\n",
    "# -------------------------\n",
    "joined_df = (\n",
    "    con.alias(\"con\")\n",
    "    .join(pur.alias(\"pur\"), \"consumer_id\", \"left\")\n",
    "    .join(avo.alias(\"avo\"), \"purchase_id\", \"left\")\n",
    "    .join(fer.alias(\"fer\"), \"purchase_id\", \"left\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Select output columns with calculations\n",
    "# -------------------------\n",
    "final_df = joined_df.select(\n",
    "    F.col(\"con.consumer_id\").alias(\"consumer_id\"),\n",
    "    F.col(\"con.sex\").alias(\"Sex\"),\n",
    "    F.col(\"con.age\").alias(\"age\"),\n",
    "    F.col(\"pur.avocado_bunch_id\").alias(\"avocado_bunch_id\"),\n",
    "    F.datediff(F.col(\"avo.sold_date\"), F.col(\"avo.born_date\")).alias(\"avocado_days_sold\"),\n",
    "    F.col(\"avo.avocado_ripe_index\").alias(\"avocado_ripe_index\"),\n",
    "    F.datediff(F.col(\"avo.picked_date\"), F.col(\"avo.born_date\")).alias(\"avocado_days_picked\"),\n",
    "    F.col(\"fer.fertilizer_type\").alias(\"fertilizer_type\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Deduplicate by natural keys (consumer_id + purchase_id + avocado_bunch_id + fertilizer_type)\n",
    "# Only deduplicate if a true duplicate exists, avoids dropping valid combinations\n",
    "# -------------------------\n",
    "dedupe_cols = [\"consumer_id\", \"avocado_bunch_id\", \"fertilizer_type\"]\n",
    "window_spec = Window.partitionBy(*dedupe_cols).orderBy(F.lit(1))\n",
    "final_df = final_df.withColumn(\"_rn\", F.row_number().over(window_spec)).filter(F.col(\"_rn\") == 1).drop(\"_rn\")\n",
    "\n",
    "# -------------------------\n",
    "# Persist Gold Delta table (authoritative)\n",
    "# -------------------------\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS gold\")\n",
    "(\n",
    "    final_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(\"gold.gold_output\")\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# CSV Output: file spec\n",
    "# -------------------------\n",
    "date_str = datetime.now().strftime(\"%Y%m%d\")\n",
    "filename = f\"target_{ITERATION}_{date_str}.csv\"\n",
    "output_staging = f\"{S3_BUCKET}/{OUTPUT_PREFIX}/{filename}.tmp/\"\n",
    "final_output_path = f\"{S3_BUCKET}/{OUTPUT_PREFIX}/{filename}\"\n",
    "\n",
    "# Order columns for CSV\n",
    "ordered_df = final_df.select(\n",
    "    \"consumer_id\",\n",
    "    \"Sex\",\n",
    "    \"age\",\n",
    "    \"avocado_days_sold\",\n",
    "    \"avocado_ripe_index\",\n",
    "    \"avocado_days_picked\",\n",
    "    \"fertilizer_type\"\n",
    ")\n",
    "\n",
    "# Only deduplicate for CSV, after ordering\n",
    "single_csv_df = ordered_df.dropDuplicates()\n",
    "\n",
    "# -------------------------\n",
    "# Write CSV\n",
    "# -------------------------\n",
    "try:\n",
    "    single_csv_df.write \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"sep\", \"|\") \\\n",
    "        .option(\"quote\", '\"') \\\n",
    "        .option(\"escape\", '\"') \\\n",
    "        .option(\"encoding\", \"ASCII\") \\\n",
    "        .option(\"lineSep\", \"\\n\") \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .csv(output_staging)\n",
    "\n",
    "    # Move part file to final CSV location\n",
    "    files = dbutils.fs.ls(output_staging)\n",
    "    if not files:\n",
    "        raise Exception(f\"No files found in staging path: {output_staging}\")\n",
    "\n",
    "    part_file = next((f.path for f in files if re.search(r\"part-.*\\.csv$\", f.name)), None)\n",
    "    if part_file is None:\n",
    "        raise Exception(f\"No part CSV found in {output_staging}; list: {[f.name for f in files]}\")\n",
    "\n",
    "    # Remove existing CSV if present\n",
    "    try:\n",
    "        dbutils.fs.rm(final_output_path)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    dbutils.fs.mv(part_file, final_output_path)\n",
    "    dbutils.fs.rm(output_staging, recurse=True)\n",
    "    print(f\"Wrote CSV to: {final_output_path}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error writing CSV: {e}\")\n",
    "    spark.sql(\"SELECT * FROM gold.gold_output\").show(100, truncate=False)\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aa16da1-0b61-49b2-8da2-3f67359ce39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5680097301181567,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "3_gold_generate.py",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
